
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cryptocurrency Arbitrage Prediction Model\n",
    "\n",
    "This notebook demonstrates how to build a model to predict arbitrage opportunities between cryptocurrency exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to MongoDB to retrieve historical price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT timestamp, symbol, exchange, price, volume\nFROM historical_prices\nWHERE symbol = 'BTC/USDT' AND timestamp > NOW() - INTERVAL '30 days'\nORDER BY timestamp\n': relation \"historical_prices\" does not exist\nLINE 3: FROM historical_prices\n             ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mUndefinedTable\u001b[0m: relation \"historical_prices\" does not exist\nLINE 3: FROM historical_prices\n             ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124mSELECT timestamp, symbol, exchange, price, volume\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124mFROM historical_prices\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124mWHERE symbol = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBTC/USDT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m AND timestamp > NOW() - INTERVAL \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m30 days\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124mORDER BY timestamp\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Read data into pandas DataFrame\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT timestamp, symbol, exchange, price, volume\nFROM historical_prices\nWHERE symbol = 'BTC/USDT' AND timestamp > NOW() - INTERVAL '30 days'\nORDER BY timestamp\n': relation \"historical_prices\" does not exist\nLINE 3: FROM historical_prices\n             ^\n"
     ]
    }
   ],
   "source": [
    "# Connect to MongoDB\n",
    "try:\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['crypto_arbitrage']\n",
    "    collection = db['price_data']\n",
    "    \n",
    "    # Query to get historical price data\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$match\": {\n",
    "                \"symbol\": \"BTC/USDT\",\n",
    "                \"timestamp\": {\"$gt\": datetime.now() - timedelta(days=30)}\n",
    "            }\n",
    "        },\n",
    "        {\"$sort\": {\"timestamp\": 1}}\n",
    "    ]\n",
    "    \n",
    "    cursor = collection.aggregate(pipeline)\n",
    "    data = list(cursor)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # If no data available, create mock data for demonstration\n",
    "    if len(df) == 0:\n",
    "        print(\"No data found in MongoDB. Creating mock data for demonstration.\")\n",
    "        mock_data = []\n",
    "        base_price = 60000\n",
    "        timestamp = datetime.now() - timedelta(days=30)\n",
    "        exchanges = ['binance', 'kucoin', 'bybit', 'okx']\n",
    "        \n",
    "        for day in range(30):\n",
    "            daily_timestamp = timestamp + timedelta(days=day)\n",
    "            for hour in range(24):\n",
    "                hourly_timestamp = daily_timestamp + timedelta(hours=hour)\n",
    "                for exchange in exchanges:\n",
    "                    # Add some random variation to price\n",
    "                    price = base_price * (1 + np.random.normal(0, 0.01))\n",
    "                    volume = np.random.randint(10000, 100000)\n",
    "                    \n",
    "                    mock_data.append({\n",
    "                        'timestamp': hourly_timestamp,\n",
    "                        'symbol': 'BTC/USDT',\n",
    "                        'exchange': exchange,\n",
    "                        'price': price,\n",
    "                        'volume': volume\n",
    "                    })\n",
    "                    \n",
    "                # Slightly adjust base price for next hour\n",
    "                base_price = base_price * (1 + np.random.normal(0, 0.001))\n",
    "                \n",
    "        df = pd.DataFrame(mock_data)\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MongoDB: {e}\")\n",
    "    print(\"Creating mock data for demonstration.\")\n",
    "    \n",
    "    # Create mock data\n",
    "    mock_data = []\n",
    "    base_price = 60000\n",
    "    timestamp = datetime.now() - timedelta(days=30)\n",
    "    exchanges = ['binance', 'kucoin', 'bybit', 'okx']\n",
    "    \n",
    "    for day in range(30):\n",
    "        daily_timestamp = timestamp + timedelta(days=day)\n",
    "        for hour in range(24):\n",
    "            hourly_timestamp = daily_timestamp + timedelta(hours=hour)\n",
    "            for exchange in exchanges:\n",
    "                # Add some random variation to price\n",
    "                price = base_price * (1 + np.random.normal(0, 0.01))\n",
    "                volume = np.random.randint(10000, 100000)\n",
    "                \n",
    "                mock_data.append({\n",
    "                    'timestamp': hourly_timestamp,\n",
    "                    'symbol': 'BTC/USDT',\n",
    "                    'exchange': exchange,\n",
    "                    'price': price,\n",
    "                    'volume': volume\n",
    "                })\n",
    "                \n",
    "            # Slightly adjust base price for next hour\n",
    "            base_price = base_price * (1 + np.random.normal(0, 0.001))\n",
    "            \n",
    "    df = pd.DataFrame(mock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display basic information about the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Display basic information about the dataset\n",
    "df = pd.read_sql(query, conn)\n",
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check for missing values\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fill missing values if needed\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfillna(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mffill\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values if needed\n",
    "df = df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with exchanges as columns and timestamp as index\n",
    "pivot_df = df.pivot_table(index='timestamp', columns='exchange', values='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price differences between exchanges\n",
    "plt.figure(figsize=(14, 7))\n",
    "for exchange in pivot_df.columns:\n",
    "    plt.plot(pivot_df.index, pivot_df[exchange], label=exchange)\n",
    "plt.title('BTC/USDT Price by Exchange')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Price (USDT)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Arbitrage Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price differences between exchanges\n",
    "exchanges = pivot_df.columns\n",
    "arbitrage_df = pd.DataFrame(index=pivot_df.index)\n",
    "\n",
    "for i in range(len(exchanges)):\n",
    "    for j in range(i+1, len(exchanges)):\n",
    "        exchange1 = exchanges[i]\n",
    "        exchange2 = exchanges[j]\n",
    "        \n",
    "        # Calculate percentage difference\n",
    "        diff_col = f\"{exchange1}_vs_{exchange2}\"\n",
    "        arbitrage_df[diff_col] = ((pivot_df[exchange2] - pivot_df[exchange1]) / pivot_df[exchange1]) * 100\n",
    "\n",
    "# Display the arbitrage opportunities\n",
    "arbitrage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize arbitrage opportunities over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "for col in arbitrage_df.columns:\n",
    "    plt.plot(arbitrage_df.index, arbitrage_df[col], label=col)\n",
    "plt.title('Arbitrage Opportunities Over Time')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Price Difference (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample data to hourly intervals for consistency\n",
    "hourly_df = arbitrage_df.resample('1H').mean().fillna(method='ffill')\n",
    "\n",
    "# Create features for time of day, day of week, etc.\n",
    "hourly_df['hour'] = hourly_df.index.hour\n",
    "hourly_df['day_of_week'] = hourly_df.index.dayofweek\n",
    "hourly_df['day_of_month'] = hourly_df.index.day\n",
    "\n",
    "# Create lag features\n",
    "for col in arbitrage_df.columns:\n",
    "    for lag in [1, 3, 6, 12, 24]:\n",
    "        hourly_df[f\"{col}_lag_{lag}\"] = hourly_df[col].shift(lag)\n",
    "        \n",
    "# Create rolling statistics\n",
    "for col in arbitrage_df.columns:\n",
    "    for window in [6, 12, 24]:\n",
    "        hourly_df[f\"{col}_mean_{window}\"] = hourly_df[col].rolling(window=window).mean()\n",
    "        hourly_df[f\"{col}_std_{window}\"] = hourly_df[col].rolling(window=window).std()\n",
    "        hourly_df[f\"{col}_max_{window}\"] = hourly_df[col].rolling(window=window).max()\n",
    "        hourly_df[f\"{col}_min_{window}\"] = hourly_df[col].rolling(window=window).min()\n",
    "\n",
    "# Drop rows with NaN values\n",
    "hourly_df = hourly_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one arbitrage pair to predict\n",
    "target_col = arbitrage_df.columns[0]  # e.g., 'binance_vs_kucoin'\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = hourly_df.drop(arbitrage_df.columns, axis=1)\n",
    "y = hourly_df[target_col]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Random Forest - MSE: {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "print(f\"Random Forest - MAE: {mean_absolute_error(y_test, y_pred_rf)}\")\n",
    "print(f\"Random Forest - R²: {r2_score(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"Gradient Boosting - MSE: {mean_squared_error(y_test, y_pred_gb)}\")\n",
    "print(f\"Gradient Boosting - MAE: {mean_absolute_error(y_test, y_pred_gb)}\")\n",
    "print(f\"Gradient Boosting - R²: {r2_score(y_test, y_pred_gb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize and Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(y_test.index, y_test.values, label='Actual')\n",
    "plt.plot(y_test.index, y_pred_rf, label='Random Forest')\n",
    "plt.plot(y_test.index, y_pred_gb, label='Gradient Boosting')\n",
    "plt.title(f'Actual vs Predicted Arbitrage Opportunities - {target_col}')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Arbitrage Difference (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_rf': rf_model.feature_importances_,\n",
    "    'importance_gb': gb_model.feature_importances_\n",
    "}).sort_values('importance_rf', ascending=False)\n",
    "\n",
    "# Plot top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance_rf', y='feature', data=feature_importances.head(15))\n",
    "plt.title('Top 15 Feature Importances - Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and scaler\n",
    "if r2_score(y_test, y_pred_rf) > r2_score(y_test, y_pred_gb):\n",
    "    best_model = rf_model\n",
    "    print(\"Saving Random Forest model\")\n",
    "else:\n",
    "    best_model = gb_model\n",
    "    print(\"Saving Gradient Boosting model\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Save model and scaler\n",
    "model_filename = f\"models/arbitrage_model_{target_col.replace('/', '_')}.pkl\"\n",
    "scaler_filename = f\"models/arbitrage_scaler_{target_col.replace('/', '_')}.pkl\"\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "    \n",
    "with open(scaler_filename, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "    \n",
    "print(f\"Model saved to {model_filename}\")\n",
    "print(f\"Scaler saved to {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature data for next 24 hours\n",
    "last_time = hourly_df.index[-1]\n",
    "future_times = [last_time + timedelta(hours=i+1) for i in range(24)]\n",
    "future_df = pd.DataFrame(index=future_times)\n",
    "\n",
    "# Add time-based features\n",
    "future_df['hour'] = future_df.index.hour\n",
    "future_df['day_of_week'] = future_df.index.dayofweek\n",
    "future_df['day_of_month'] = future_df.index.day\n",
    "\n",
    "# Initialize with last known values\n",
    "for col in hourly_df.columns:\n",
    "    if col not in ['hour', 'day_of_week', 'day_of_month']:\n",
    "        future_df[col] = hourly_df[col].iloc[-1]\n",
    "\n",
    "# Scale future data\n",
    "future_features = future_df[X.columns]\n",
    "future_scaled = scaler.transform(future_features)\n",
    "\n",
    "# Make predictions\n",
    "future_predictions = best_model.predict(future_scaled)\n",
    "\n",
    "# Add predictions to the future DataFrame\n",
    "future_df['prediction'] = future_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(hourly_df.index[-48:], hourly_df[target_col][-48:], label='Historical')\n",
    "plt.plot(future_df.index, future_df['prediction'], label='Predicted', linestyle='--')\n",
    "plt.axvline(x=last_time, color='r', linestyle='-', alpha=0.3, label='Now')\n",
    "plt.title(f'Predicted Arbitrage Opportunities - {target_col}')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Arbitrage Difference (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Identify Optimal Trading Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold for profitable arbitrage (accounting for fees)\n",
    "threshold = 0.5  # 0.5% profit after fees\n",
    "\n",
    "# Identify predicted opportunities above threshold\n",
    "profitable_times = future_df[future_df['prediction'] > threshold]\n",
    "\n",
    "print(f\"Predicted profitable arbitrage opportunities for {target_col}:\")\n",
    "if len(profitable_times) > 0:\n",
    "    for idx, row in profitable_times.iterrows():\n",
    "        print(f\"Time: {idx}, Expected Profit: {row['prediction']:.3f}%\")\n",
    "else:\n",
    "    print(\"No profitable opportunities predicted in the next 24 hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Trading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting function\n",
    "def backtest_strategy(df, target_column, threshold=0.5, investment=1000, fee_rate=0.001):\n",
    "    results = {\n",
    "        'trades': [],\n",
    "        'total_profit': 0,\n",
    "        'win_rate': 0,\n",
    "        'avg_profit': 0\n",
    "    }\n",
    "    \n",
    "    # Find opportunities\n",
    "    opportunities = df[df[target_column] > threshold]\n",
    "    \n",
    "    successful_trades = 0\n",
    "    total_profit_pct = 0\n",
    "    \n",
    "    for idx, row in opportunities.iterrows():\n",
    "        # Calculate expected profit\n",
    "        expected_profit_pct = row[target_column]\n",
    "        \n",
    "        # Calculate actual profit (considering fees)\n",
    "        actual_profit_pct = expected_profit_pct - (fee_rate * 200)  # 2 trades (buy and sell)\n",
    "        profit_amount = investment * (actual_profit_pct / 100)\n",
    "        \n",
    "        # Log trade\n",
    "        trade = {\n",
    "            'timestamp': idx,\n",
    "            'expected_profit_pct': expected_profit_pct,\n",
    "            'actual_profit_pct': actual_profit_pct,\n",
    "            'profit_amount': profit_amount,\n",
    "            'successful': actual_profit_pct > 0\n",
    "        }\n",
    "        \n",
    "        results['trades'].append(trade)\n",
    "        results['total_profit'] += profit_amount\n",
    "        \n",
    "        if actual_profit_pct > 0:\n",
    "            successful_trades += 1\n",
    "            total_profit_pct += actual_profit_pct\n",
    "    \n",
    "    # Calculate statistics\n",
    "    num_trades = len(results['trades'])\n",
    "    if num_trades > 0:\n",
    "        results['win_rate'] = successful_trades / num_trades * 100\n",
    "        results['avg_profit'] = total_profit_pct / num_trades if successful_trades > 0 else 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run backtest on historical data\n",
    "backtest_results = backtest_strategy(hourly_df, target_col, threshold=0.5)\n",
    "\n",
    "# Print results\n",
    "print(f\"Backtest Results for {target_col}:\")\n",
    "print(f\"Number of trades: {len(backtest_results['trades'])}\")\n",
    "print(f\"Total profit: ${backtest_results['total_profit']:.2f}\")\n",
    "print(f\"Win rate: {backtest_results['win_rate']:.2f}%\")\n",
    "print(f\"Average profit per successful trade: {backtest_results['avg_profit']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "- We've built a model to predict arbitrage opportunities between cryptocurrency exchanges\n",
    "- The model achieved reasonable accuracy in predicting price differences\n",
    "- Backtesting showed profitable trading opportunities\n",
    "\n",
    "### Next Steps\n",
    "1. Expand the model to more trading pairs\n",
    "2. Implement real-time prediction pipeline\n",
    "3. Add liquidity analysis to ensure trades can be executed\n",
    "4. Develop automated trading execution system\n",
    "5. Add risk management and position sizing logic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
